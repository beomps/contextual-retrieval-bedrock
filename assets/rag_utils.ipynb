{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import uuid\n",
        "import json\n",
        "import boto3\n",
        "import logging\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from botocore.config import Config\n",
        "import pdfplumber\n",
        "from dotenv import load_dotenv\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "\n",
        "load_dotenv()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class OpenSearch_Manager:\n",
        "    def __init__(self, prefix='aws_'):\n",
        "        self.client = self._init_opensearch()\n",
        "        self.prefix = prefix \n",
        "        self.index_list = self._get_indices()\n",
        "\n",
        "    def _init_opensearch(self):\n",
        "        try:\n",
        "            host = os.getenv('OPENSEARCH_HOST')\n",
        "            user = os.getenv('OPENSEARCH_USER')\n",
        "            password = os.getenv('OPENSEARCH_PASSWORD')\n",
        "            region = os.getenv('OPENSEARCH_REGION')\n",
        "            client = OpenSearch(\n",
        "                hosts = [{'host': host.replace(\"https://\", \"\"), 'port': 443}],\n",
        "                http_auth = (user, password),\n",
        "                use_ssl = True,\n",
        "                verify_certs = True,\n",
        "                connection_class = RequestsHttpConnection\n",
        "            )\n",
        "            return client\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing OpenSearch: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _get_mappings(self):\n",
        "        mapping = {\n",
        "            \"settings\": {\n",
        "                \"index.knn\": True,\n",
        "                \"index.knn.algo_param.ef_search\": 512\n",
        "            },\n",
        "            \"mappings\": {\n",
        "                \"properties\": {\n",
        "                    \"metadata\": {\n",
        "                        \"properties\": {\n",
        "                            \"source\": {\n",
        "                                \"type\": \"keyword\"\n",
        "                            },\n",
        "                            \"doc_id\": {\n",
        "                                \"type\": \"keyword\"\n",
        "                            },\n",
        "                            \"timestamp\": {\n",
        "                                \"type\": \"date\"\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"analyzer\": \"standard\"\n",
        "                    },\n",
        "                    \"content_embedding\": {\n",
        "                        \"type\": \"knn_vector\",\n",
        "                        \"dimension\": 1024,\n",
        "                        \"method\": {\n",
        "                            \"engine\": \"faiss\",\n",
        "                            \"name\": \"hnsw\",\n",
        "                            \"parameters\": {\n",
        "                                \"ef_construction\": 512,\n",
        "                                \"m\": 16\n",
        "                            },\n",
        "                            \"space_type\": \"l2\"\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return mapping\n",
        "\n",
        "    def _get_indices(self):\n",
        "        try:\n",
        "            index_pattern = f\"{self.prefix}*\" if self.prefix else \"*\"\n",
        "            indices = self.client.cat.indices(index=index_pattern, format=\"json\")\n",
        "            return [index['index'][len(self.prefix):] for index in indices]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching indices: {e}\")\n",
        "            return []\n",
        "\n",
        "    def refresh_index_list(self):\n",
        "        self.index_list = self._get_indices()\n",
        "\n",
        "    def create_index(self, index_name, index_action):\n",
        "        full_index_name = f\"{self.prefix}{index_name}\"\n",
        "        try:\n",
        "            if index_action == \"Overwrite existing index\":\n",
        "                if self.client.indices.exists(index=full_index_name):\n",
        "                    self.client.indices.delete(index=full_index_name)\n",
        "                    logger.info(f\"Existing index '{full_index_name}' deleted.\")\n",
        "\n",
        "                mapping = self._get_mappings()\n",
        "                self.client.indices.create(index=full_index_name, body=mapping)\n",
        "                logger.info(f\"Index '{full_index_name}' created successfully.\")\n",
        "\n",
        "            elif index_action == \"Append to existing index\":\n",
        "                if not self.client.indices.exists(index=full_index_name):\n",
        "                    mapping = self._get_mappings()\n",
        "                    self.client.indices.create(index=full_index_name, body=mapping)\n",
        "                    logger.info(f\"Index '{full_index_name}' did not exist. Created new index.\")\n",
        "                else:\n",
        "                    logger.info(f\"Index '{full_index_name}' already exists. Ready to append data.\")\n",
        "\n",
        "            else:\n",
        "                logger.error(f\"Invalid index_action: {index_action}\")\n",
        "                return False\n",
        "\n",
        "            self.refresh_index_list()\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error performing {index_action} action on index '{full_index_name}': {e}\")\n",
        "            return False\n",
        "\n",
        "    def _search(self, query, index_name, top_n=80):\n",
        "        try:\n",
        "            response = self.client.search(index=index_name, body=query)\n",
        "            results = []\n",
        "            for hit in response['hits']['hits']:\n",
        "                result = {\n",
        "                    \"content\": hit['_source'][\"content\"],\n",
        "                    \"score\": hit['_score'],\n",
        "                    \"metadata\": hit['_source']['metadata'],\n",
        "                    \"search_method\": query['query'].get('knn', 'bm25')\n",
        "                }\n",
        "                results.append(result)\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logger.error(f\"An error occurred during search: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_by_knn(self, vector, index_name, top_n=80):\n",
        "        query = {\n",
        "            \"size\": top_n,\n",
        "            \"_source\": [\"content\", \"metadata\"],\n",
        "            \"query\": {\n",
        "                \"knn\": {\n",
        "                    \"content_embedding\": {\n",
        "                        \"vector\": vector,\n",
        "                        \"k\": top_n\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        results = self._search(query, index_name, top_n)\n",
        "        for result in results:\n",
        "            result['search_method'] = 'knn'\n",
        "        return results\n",
        "\n",
        "    def search_by_bm25(self, query_text, index_name, top_n=80):\n",
        "        query = {\n",
        "            \"size\": top_n,\n",
        "            \"_source\": [\"content\", \"metadata\"],\n",
        "            \"query\": {\n",
        "                \"match\": {\n",
        "                    \"content\": {\n",
        "                        \"query\": query_text,\n",
        "                        \"operator\": \"or\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return self._search(query, index_name, top_n)\n",
        "\n",
        "    def _rerank_documents(self, question, documents, top_k=20):\n",
        "        rerank_api_url = os.getenv('RERANK_API_URL')\n",
        "        payload = {\n",
        "            \"documents\": documents,\n",
        "            \"query\": question,\n",
        "            \"rank_fields\": [\"content\"],\n",
        "            \"top_n\": top_k\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(rerank_api_url, json=payload, headers=headers)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result \n",
        "            else:\n",
        "                logger.error(f\"Error: API failed (status code: {response.status_code})\")\n",
        "                logger.error(f\"response: {response.text}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in _rerank_documents: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def search_by_rank_fusion(self, query_text, vector, index_name, initial_search_results=160, hybrid_score_filter=40, final_reranked_results=20, knn_weight=0.6):\n",
        "        half_initial = initial_search_results // 2\n",
        "        knn_results = self.search_by_knn(vector, index_name, half_initial)\n",
        "        bm25_results = self.search_by_bm25(query_text, index_name, half_initial)\n",
        "\n",
        "        bm25_weight = 1 - knn_weight\n",
        "\n",
        "        def _normalize_and_weight_score(results, weight):\n",
        "            if not results:\n",
        "                return results\n",
        "            min_score = min(r['score'] for r in results)\n",
        "            max_score = max(r['score'] for r in results)\n",
        "            score_range = max_score - min_score\n",
        "            if score_range == 0:\n",
        "                return results\n",
        "            for r in results:\n",
        "                r['normalized_score'] = ((r['score'] - min_score) / score_range) * weight\n",
        "            return results\n",
        "\n",
        "        knn_results = _normalize_and_weight_score(knn_results, knn_weight)\n",
        "        bm25_results = _normalize_and_weight_score(bm25_results, bm25_weight)\n",
        "\n",
        "        # Combine results and calculate hybrid score\n",
        "        combined_results = {}\n",
        "        for result in knn_results + bm25_results:\n",
        "            chunk_id = result['metadata'].get('chunk_id', result['content']) \n",
        "            if chunk_id not in combined_results:\n",
        "                combined_results[chunk_id] = result.copy()\n",
        "                combined_results[chunk_id]['hybrid_score'] = result.get('normalized_score', 0)\n",
        "                combined_results[chunk_id]['search_methods'] = [result['search_method']]\n",
        "            else:\n",
        "                combined_results[chunk_id]['hybrid_score'] += result.get('normalized_score', 0)\n",
        "                if result['search_method'] not in combined_results[chunk_id]['search_methods']:\n",
        "                    combined_results[chunk_id]['search_methods'].append(result['search_method'])\n",
        "\n",
        "        # Convert back to list and sort by hybrid score\n",
        "        results_list = list(combined_results.values())\n",
        "        results_list.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
        "        hybrid_results = results_list[:hybrid_score_filter]\n",
        "\n",
        "        # Prepare documents for reranking\n",
        "        documents_for_rerank = [\n",
        "            {\"content\": doc['content'], \"metadata\": doc['metadata']} for doc in hybrid_results\n",
        "        ]\n",
        "\n",
        "        # Rerank the documents -> return ranked indices\n",
        "        reranked_results = self._rerank_documents(query_text, documents_for_rerank, final_reranked_results)\n",
        "\n",
        "        # Prepare final results\n",
        "        if reranked_results and isinstance(reranked_results, dict) and 'results' in reranked_results:\n",
        "            final_results = []\n",
        "            for reranked_doc in reranked_results['results']:\n",
        "                if isinstance(reranked_doc, dict) and 'index' in reranked_doc and 'relevance_score' in reranked_doc:\n",
        "                    index = reranked_doc['index']\n",
        "                    if 0 <= index < len(hybrid_results):\n",
        "                        original_doc = hybrid_results[index]\n",
        "                        final_doc = {\n",
        "                            \"content\": original_doc[\"content\"],\n",
        "                            'metadata': original_doc['metadata'],\n",
        "                            'score': reranked_doc['relevance_score'], \n",
        "                            'hybrid_score': original_doc['hybrid_score'],\n",
        "                            'search_methods': original_doc['search_methods']\n",
        "                        }   \n",
        "                        final_results.append(final_doc)\n",
        "                else:\n",
        "                    logger.warning(f\"Unexpected reranked document format: {reranked_doc}\")\n",
        "\n",
        "            final_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        else:\n",
        "            logger.warning(\"Reranking failed or returned unexpected format. Using hybrid results.\")\n",
        "            final_results = [{\n",
        "                \"content\": doc[\"content\"],\n",
        "                'metadata': doc['metadata'],\n",
        "                'score': doc['hybrid_score'],\n",
        "                'hybrid_score': doc['hybrid_score'],\n",
        "                'search_methods': doc['search_methods']\n",
        "            } for doc in hybrid_results[:final_reranked_results]]\n",
        "        \n",
        "        return final_results\n",
        "\n",
        "class Context_Processor:\n",
        "    def __init__(self, os_manager, embed_model, bedrock_region, index_name, chunk_size, overlap, use_context_retrieval, context_model=None, max_document_len=None):\n",
        "        self.os_manager = os_manager\n",
        "        self.embed_model = embed_model\n",
        "        self.index_name = index_name\n",
        "        self.chunk_size = chunk_size\n",
        "        self.use_context_retrieval = use_context_retrieval \n",
        "        self.overlap = overlap \n",
        "        self.context_model = context_model \n",
        "        self.max_document_len = max_document_len\n",
        "        self.bedrock_client = self._init_bedrock_client(bedrock_region)\n",
        "\n",
        "    def _init_bedrock_client(self, bedrock_region):\n",
        "        retry_config = Config(\n",
        "            region_name=bedrock_region,\n",
        "            retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
        "        )\n",
        "        return boto3.client(\"bedrock-runtime\", config=retry_config)\n",
        "\n",
        "    def _split_into_chunks(self, text, chunk_size, overlap):\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            if end >= len(text):\n",
        "                chunks.append(text[start:].strip())\n",
        "                break\n",
        "\n",
        "            next_newline = text.find('\\n', end)\n",
        "            next_sentence = text.find('. ', end)\n",
        "            next_word = text.find(' ', end)\n",
        "\n",
        "            separators = [s for s in [next_newline, next_sentence, next_word] if s != -1]\n",
        "            if separators:\n",
        "                end = min(separators)\n",
        "                if next_newline == end:\n",
        "                    end += 1\n",
        "                elif next_sentence == end:\n",
        "                    end += 2\n",
        "                else:\n",
        "                    end += 1\n",
        "\n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "\n",
        "            start = max(end - overlap, start + 1)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _load_and_split(self, file, start_page, end_page):\n",
        "        documents = []\n",
        "        full_text = \"\"\n",
        "        with pdfplumber.open(io.BytesIO(file.getvalue())) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            end_page = min(end_page or total_pages, total_pages)\n",
        "\n",
        "            for page_num in range(start_page - 1, end_page):\n",
        "                text = pdf.pages[page_num].extract_text()\n",
        "                text = re.sub(r'\\s+', ' ', text).strip()\n",
        "                full_text += text + \" \"\n",
        "\n",
        "        if self.use_context_retrieval and self.max_document_len:\n",
        "            logger.info(f\"Starting document splitting with max_document_len: {self.max_document_len}\")\n",
        "            doc_chunks = self._split_into_chunks(full_text, self.max_document_len, 0)\n",
        "        else:\n",
        "            doc_chunks = [full_text]\n",
        "            logger.info(f\"Document splitting has been skipped.\")\n",
        "        \n",
        "        for doc_index, doc_chunk in enumerate(doc_chunks):\n",
        "            doc_id = f\"doc_{doc_index+1}\"\n",
        "            chunks = self._split_into_chunks(doc_chunk, self.chunk_size, self.overlap)\n",
        "\n",
        "            document_chunks = [\n",
        "                {\n",
        "                    \"chunk_id\": f\"{doc_id}_chunk_{chunk_index}\",\n",
        "                    \"original_index\": chunk_index,\n",
        "                    \"content\": chunk\n",
        "                } for chunk_index, chunk in enumerate(chunks)\n",
        "            ]\n",
        "\n",
        "            documents.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"original_uuid\": str(uuid.uuid4()),\n",
        "                \"content\": doc_chunk,\n",
        "                \"chunks\": document_chunks\n",
        "            })\n",
        "            logger.info(f\"Chunking of doc_{doc_index+1} has been completed.\")\n",
        "        \n",
        "        return documents\n",
        "\n",
        "    def _save_documents_to_json(self, documents, filename):\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(documents, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def _situate_document(self, documents):\n",
        "        logger.info(f\"Starting to situate {len(documents)} documents\")\n",
        "        total_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
        "        documents_token_usage = {}\n",
        "\n",
        "        sys_prompt = [{\"text\": \"\"\"\n",
        "        You're an expert at providing a succinct context, targeted for specific text chunks.\n",
        "\n",
        "        <instruction>\n",
        "        - Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
        "        - Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
        "        - Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
        "        - If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
        "        </instruction>\n",
        "        \"\"\"}]\n",
        "\n",
        "        for doc_index, document in enumerate(documents, 1):\n",
        "            logger.info(f\"Processing document {doc_index}/{len(documents)}\")\n",
        "            doc_content = document['content']\n",
        "\n",
        "            doc_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
        "            \n",
        "            for chunk in document['chunks']:                \n",
        "                document_context_prompt = f\"\"\"\n",
        "                <document>\n",
        "                {doc_content}\n",
        "                </document>\n",
        "                \"\"\"\n",
        "\n",
        "                chunk_content = chunk['content']\n",
        "                chunk_context_prompt = f\"\"\"\n",
        "                Here is the chunk we want to situate within the whole document:\n",
        "\n",
        "                <chunk>\n",
        "                {chunk_content}\n",
        "                </chunk>\n",
        "\n",
        "                Skip the preamble and only provide the consise context.\n",
        "                \"\"\"\n",
        "                usr_prompt = [{\n",
        "                        \"role\": \"user\", \n",
        "                        \"content\": [\n",
        "                            {\"text\": document_context_prompt},\n",
        "                            {\"text\": chunk_context_prompt}\n",
        "                        ]\n",
        "                    }]\n",
        "\n",
        "                temperature = 0.0\n",
        "                top_p = 0.5\n",
        "                inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
        "\n",
        "                try:\n",
        "                    response = self.bedrock_client.converse(\n",
        "                        modelId=self.context_model,\n",
        "                        messages=usr_prompt, \n",
        "                        system=sys_prompt,\n",
        "                        inferenceConfig=inference_config,\n",
        "                    )\n",
        "                    situated_context = response['output']['message']['content'][0]['text'].strip()\n",
        "                    chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk['content']}\"\n",
        "\n",
        "                    if 'usage' in response:\n",
        "                        usage = response['usage']\n",
        "                        for key in ['inputTokens', 'outputTokens', 'totalTokens']:\n",
        "                            doc_token_usage[key] += usage.get(key, 0)\n",
        "                            total_token_usage[key] += usage.get(key, 0)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error generating context for chunk: {e}\")\n",
        "\n",
        "            documents_token_usage[f\"document_{doc_index}\"] = doc_token_usage\n",
        "            logger.info(f\"Completed processing document {doc_index}/{len(documents)}\")\n",
        "            logger.info(f\"Document {doc_index} token usage - Input: {doc_token_usage['inputTokens']}, \"\n",
        "                        f\"Output: {doc_token_usage['outputTokens']}, Total: {doc_token_usage['totalTokens']}\")\n",
        "\n",
        "        logger.info(f\"Total token usage - Input: {total_token_usage['inputTokens']}, \"\n",
        "                    f\"Output: {total_token_usage['outputTokens']}, \"\n",
        "                    f\"Total: {total_token_usage['totalTokens']}\")\n",
        "\n",
        "        token_usage_data = {\n",
        "            \"total_usage\": total_token_usage,\n",
        "            \"documents_usage\": documents_token_usage\n",
        "        }\n",
        "\n",
        "        with open(f\"{self.index_name}_token_usage.json\", 'w') as f:\n",
        "            json.dump(token_usage_data, f, indent=4)\n",
        "        logger.info(f\"Token usage saved to {self.index_name}_token_usage.json\")\n",
        "\n",
        "        return documents\n",
        "    \n",
        "\n",
        "    def _embed_document(self, text):\n",
        "        try:\n",
        "            response = self.bedrock_client.invoke_model(\n",
        "                modelId=self.embed_model,\n",
        "                body=json.dumps({\"inputText\": text})\n",
        "            )\n",
        "            return json.loads(response['body'].read())['embedding']\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error embedding document: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def _embed_and_store(self, source_file_name):\n",
        "        try:\n",
        "            with open(f\"{self.index_name}_chunks.json\", 'r', encoding='utf-8') as f:\n",
        "                documents = json.load(f)\n",
        "            \n",
        "            embedded_documents = []\n",
        "\n",
        "            for document in documents:\n",
        "                doc_id = document['doc_id']\n",
        "                embedded_chunks = []\n",
        "\n",
        "                for chunk in document['chunks']:\n",
        "                    context = chunk['content']\n",
        "                    chunk_embedding = self._embed_document(context)\n",
        "                    if chunk_embedding:\n",
        "                        chunk_id = chunk['chunk_id']\n",
        "                        _id = f\"{doc_id}_{chunk_id}\"\n",
        "                        embedded_chunk = {\n",
        "                            \"metadata\": {\n",
        "                                \"source\": source_file_name, \n",
        "                                \"doc_id\": doc_id,\n",
        "                                \"chunk_id\": chunk_id,\n",
        "                                \"timestamp\": datetime.now().isoformat()\n",
        "                            },\n",
        "                            \"content\": chunk['content'],\n",
        "                            \"content_embedding\": chunk_embedding\n",
        "                        }\n",
        "                        embedded_chunks.append(embedded_chunk)\n",
        "\n",
        "                        self.os_manager.client.index(\n",
        "                            index=f\"aws_{self.index_name}\",\n",
        "                            body=embedded_chunk\n",
        "                        )\n",
        "                        \n",
        "                    embedded_documents.append({\n",
        "                        \"_id\": _id,\n",
        "                        \"embedded_chunks\": embedded_chunks\n",
        "                    })\n",
        "                    \n",
        "            print(f\"Successfully embedded and stored documents in index 'aws_{self.index_name}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding and storing documents: {e}\")\n",
        "\n",
        "\n",
        "    def process_file(self, file, index_action, start_page=1, end_page=None):\n",
        "        documents = self._load_and_split(file, start_page, end_page)\n",
        "        if self.use_context_retrieval:\n",
        "            documents = self._situate_document(documents)\n",
        "        self._save_documents_to_json(documents, f\"{self.index_name}_chunks.json\")\n",
        "        self.os_manager.create_index(self.index_name, index_action)\n",
        "        self._embed_and_store(file.name)\n",
        "        \n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}